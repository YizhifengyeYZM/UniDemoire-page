<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <!-- <meta name="description" content="DESCRIPTION META TAG"> -->

    <meta property='og:title' content='UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis'/>
    <meta property='og:url' content='https://mingukkang.github.io/Diffusion2GAN/'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png"> -->
    <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <!-- Keywords for your paper to be indexed by-->
    <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>UniDemoiré</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">
</head>

<body>
  <section class="hero banner">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mingukkang.github.io/">Zemin Yang</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://richzhang.github.io/">Yujing Sun</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.connellybarnes.com/work/">Xidong Peng</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.adobe.com/person/sylvain-paris/">Siu Ming Yiu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://suhakwak.github.io/">Yuexin Ma</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShanghaiTech University,</span>
            <span class="author-block"><sup>2</sup>The University of Hong Kong,</span>
          </div>

          <div class="is-size-5 publication-venue">
            in AAAI 2025
          </div>

          <!-- Paper pdf. -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="static/paper/unidemoire_main.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Appendix pdf. -->
              <span class="link-block">
                <a href="static/paper/unidemoire_appendix.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Appendix</span>
                </a>
              </span>

              <!-- Arxiv 部分. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2405.05967"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/4DVLab/UniDemoire"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>UniDemoiré</span>
                  </a>
              </span>

              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=nHav5H4uBgA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="static/slides/[PPT]Diffusion2GAN.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>


              <span class="link-block">
                <a href="static/slides/[poster]Diffusion2GAN.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->

              <!-- 引用. -->
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-4">Two-step 4K image generation using Diffusion2GAN and GigaGAN</h2>
      <div class="content has-text-justified">
        <p>
          <b>The images generated by Diffusion2GAN can be seamlessly upsampled to 4k res using GigaGAN upsampler.</b> This indicates that we can generate low-resolution preview images using Diffusion2GAN and then enhance some preferred images to 4k resolution using the GigaGAN upsampler.
          <br> 
        </p>
      </div>

      <div class="tab_container">
       
        <div id="juxtapose-embed" data-startingposition="30%" data-animate="true">
        </div>

        <div>
          <div id="juxtapose-hidden"></div>
        </div>
        
        <div id="imgtext"></div>
      </div>

      <div class="tab_row">
        <div class="tab_column">
          <img src="./static/images/1_input.jpeg"  onclick="tab_gallery_click('1');">
        </div>

        <div class="tab_column">
          <img src="./static/images/3_input.jpeg" onclick="tab_gallery_click('3');">
        </div>
        <div class="tab_column">
          <img src="./static/images/4_input.jpeg" onclick="tab_gallery_click('4');">
        </div>
        <div class="tab_column">
          <img src="./static/images/5_input.jpeg" onclick="tab_gallery_click('5');">
        </div>
        <div class="tab_column">
          <img src="./static/images/6_input.jpeg" onclick="tab_gallery_click('6');">
        </div>
        
      </div>
    </div>
  </div>

</section> -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image demoiréing poses one of the most formidable challenges in image restoration, 
            primarily due to the unpredictable and anisotropic nature of moiré patterns. 
            Limited by the quantity and diversity of training data, current methods tend to overfit to a single moiré domain, 
            resulting in performance degradation for new domains and restricting their robustness in real-world applications. 
            In this paper, we propose a universal image demoiréing solution, <strong>UniDemoiré</strong>, 
            which has superior generalization capability. Notably, 
            we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moiré images to train a universal demoiréing model. 
            Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoiréing.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">UniDemoiré Framework</h2>

        <!-- 总体 Pipeline 图 -->
        <div class="content has-text-justified">
          <p>
            The generalization ability of SOTA demoiréing models is greatly limited by the scarcity of data. 
            Therefore, we mainly face two challenges to obtain a universal model with improved generalization capability: 
              To obtain a vast amount of <b>1) diverse</b> and <b>2) realistic-looking moiré data</b>.
            Notice that traditional moiré image datasets contain real data, but continuously expanding their size to involve more diversity is extremely time-consuming and impractical. 
            While current synthesized datasets/methods struggle to synthesize realistic-looking moiré images.
          </p>
          <img src="./static/images/Pipeline.png" width="100%">
          
          <br>
        </div>

        <div class="content has-text-justified">
          <p>
            Hence, to tackle these challenges, we introduce a universal solution, <b>UniDemoiré</b>. 
            The data diversity challenge is solved by collecting a more diverse moiré pattern dataset and presenting a moiré pattern generator to increase further pattern variations. 
            Meanwhile, the data realistic-looking challenge is undertaken by a moiré image synthesis module.  
            Finally, our solution can produce realistic-looking moiré images of sufficient diversity, substantially enhancing the zero-shot and cross-domain performance of demoiréing models.
          </p>
          <br>
        </div>
 
        <!-- 1. Moire Patterns Collection -->
        <h3 class="title is-4">Moiré Pattern Dataset</h3>
        
        <div class="content has-text-justified">
          <p>
            We propose to collect a moiré pattern dataset rather than a moiré image dataset, 
            with no need for image alignment and can easily synthesize multiple moiré counterparts of a single natural image.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Samples of Moire Patterns.png" width="100%">

        </div>

        <div class="content has-text-justified">
          <p>
            A detailed comparison of ours and others is shown in the Table below:
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Dataset Comparisons.png" width="100%">
          <p>
            Comparisons of different moiré datasets. The "R" denotes the real dataset, and the "S" denotes the synthetic dataset.
          </p>
        </div>

        <h4 class="title is-5">Capturing Process</h4>
        <div class="content has-text-justified">
          <p>
            We capture videos of real-world moiré patterns on a pure white screen with a mobile phone to minimize color distortion in the moiré patterns. 
            After recording, frames are uniformly extracted from each video to constitute our dataset.
          </p>
        </div>


        <div class="content has-text-centered">
            <img src="./static/images/Data Collection.png" width="90%">
            <p>
              Data collection setup (left), 
              and examples of moiré patterns in our dataset captured at different zoom rates and screen panel (middle), and our generated patterns (right).
            </p>
        </div>

        <!-- <div class="content has-text-justified">
          <p>
            Data collection setup (left), 
            and examples of moiré patterns in our dataset captured at different zoom rates and screen panel (middle), and our generated patterns (right).
          </p>
        </div> -->

        <div class="container has-text-centered">
          <!-- 视频部分 -->
          <video width="80%" controls>
            <source src="./static/videos/captured.mp4" type="video/mp4">
          </video>
          <p>
            A video example with the settings above.
          </p>
          <br>
        </div>


        <h4 class="title is-5">Data Diversity</h4>
        <div class="content has-text-justified">
          <p>
            To enhance pattern diversity, we build our dataset by considering additional factors that influence moiré formation, 
            which were overlooked in previous moiré datasets, including <b>Zooming Rate</b>, <b>Camera Types</b>, <b>CMOS</b>, and <b>Screen Panel Types</b>. 
            Besides, we doubled the number of mobile devices and display screens compared to existing datasets.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/mobile phone.png" width="100%">
          <p>
            The mobile phone we apply to get the moiré patterns.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/screen.png" width="100%">
          <p>
            The screen we apply to get the moiré patterns.
          </p>
        </div>


        <!-- 2. Moiré Pattern Generation -->
        <h3 class="title is-4">Moiré Pattern Generation</h3>
        <div class="content has-text-justified">
          <p>
            We propose to use diffusion models to further sample more diverse moiré patterns by sufficiently learning the structural, 
            textural, and color representations of real moiré patterns.
          </p>
        </div>

        <h4 class="title is-5">Data Preprocessing</h4>

        <div class="content has-text-centered">
          <img src="./static/images/Data Preprocessing.png" width="100%">
          <p>
            Data preprocessing for moiré pattern generation.
          </p>
        </div>

        <h4 class="title is-5">Learning Moiré Patterns in the Latent Space</h4>
        <div class="content has-text-justified">
          <p>
            We notice that plenty of pixels in the moiré pattern appear pure white.
            This leads to a polarization in the pixel distribution of the moiré pattern images, 
            where informative data is concentrated in a few pixels with high values while the rest contains little information.
            
          </p>
          <p>
            Based on this observation, we choose to <b>compress the moiré pattern into the latent space</b> through an autoencoder for 
            a more compact and efficient representation of its structural, textural, and color information. 
            For better stability and controllability, we utilize the <b>Latent Diffusion Model</b> to effectively model the complex distribution of the moiré pattern in the latent space.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Generated Data.png" width="100%">
          <p>
            Visualization of sampled patches using our Moiré Pattern Generator.
          </p>
        </div>

        <!-- 3. Moiré Image Synthesis -->
        <h3 class="title is-4">Moiré Image Synthesis</h3>
        

        <div class="content has-text-centered">
          <img src="./static/images/Moire Image Synthesis.png" width="100%">
          <p>
            Overview of the Moiré Image Synthesis stage (a). 
            It involves a Moiré Image Blending module (b) for initial moiré image synthesis and a Tone Refinement Network (c) to refine for more realistic results.
          </p>
        </div>

        <div class="content has-text-justified">
          <p>
            Via data collection and generation, we obtain a vast number of diverse moiré patterns. 
            Then, we need to composite moiré patterns with clean images to form moiré images. 
            To make the synthesized images realistic-looking, We first create handcraft rules to produce initial moiré images 
            in the Moiré Image Blending (MIB) module, then design a Tone Refinement Network (TRN) to further faithfully 
            replicate the color and brightness variations observed in real scenes that cannot be fully formulated in those handcraft rules. 
            Finally, the entire network is trained using a weighted compound of perception loss, color differentiable RGB-uv histogram loss and total variation regularizer.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Synthesis Result.png" width="100%">
          <p>
            Visualization of our intermediate synthetic results.
            The final synthesis of TRN best resembles the real moiré images in contrast and brightness distortions.
          </p>
        </div>

        <!-- 4. Image Demoiréing -->
        <h3 class="title is-4">Image Demoiréing</h3>

        <div class="content has-text-justified">
          <p>
            Our contributions mainly lie in the above three stages. 
            Then, diverse and realistic-looking data synthesized by our solution can be seamlessly integrated with demoiréing models to improve their performance.
          </p>
          <br>
        </div>
      </div>
    </div>


    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>
        <br>

        <h3 class="title is-4">Zero-Shot Demoiréing with Synthesized Data Only</h3>

        <div class="content has-text-justified">
          <p>
            We first demonstrate demoiréing results on real moiré images trained on purely synthesized data by SOTA moiré synthesis methods.
          </p>
          <p>
            To avoid data overlap in training sets and test sets, we have collected a comprehensive <b>M</b>ixed <b>H</b>igh-Resolution <b>N</b>atural <b>I</b>mage <b>D</b>ataset <b>(MHRNID)</b>, 
            based on which, moiré images are synthesized for training demoiréing models.
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/MHRNID.png" width="100%">
          <p>
            Examples of the MHRNID dataset.
          </p>
        </div>

        <div class="content has-text-justified">
          <p>
            Quantitative comparisons can be found in Table below:
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Zero shot.png" width="100%">
          <p>
            Quantitative results of zero-shot demoiréing trained with synthesized data only. 
            "†" indicates UnDem uses moiré patterns retrieved from real data in TIP for inference.  
            "‡" indicates UnDem uses our generated moiré pattern for inference.
          </p>
        </div>

        <div class="content has-text-justified">
          <p>
            From the quantitative perspective, our method substantially outperforms all other approaches.
            We attribute our superiority to the diversity and realism of our synthetic data. 
            Such high-quality data by our UniDemoiré enables the demoiréing model to learn moiré characteristics better, improving performance in removing unseen moiré artifacts.
          </p>
        </div>

        <h3 class="title is-4">Cross-Dataset Evaluation</h3>

        <div class="content has-text-justified">
          <p>
            We then demonstrate our ability to improve the performance of demoiréing models across domains. 
            Quantitative results are shown in Table below:
          </p>
        </div>

        <div class="content has-text-centered">
          <img src="./static/images/Cross Dataset.png" width="100%">
          <p>
            Quantitative results of cross-dataset evaluations.
          </p>
        </div>

        <div class="content has-text-justified">
          <p>
            As shown, thanks to the realistic and diverse synthesized data, our method outperforms all previous methods across every experiment.
          </p>
        </div>

      </div>
    </div>

    

    <div class="container is-max-desktop has-text-centered">

      <h2 class="title is-3">More Qualitative Comparisons</h2>

      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-t2i1">
          <img id="myt2i0" src="./static/images/Synthesis Result Compare.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1">
          <img id="myt2i0" src="./static/images/Zero-Shot Result.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1">
          <img id="myt2i0" src="./static/images/FHDMi Result.png"
          class="interpolation-image"/>
        </div>
        <div class="item item-t2i1">
          <img id="myt2i0" src="./static/images/TIP Result.png"
          class="interpolation-image"/>
        </div>
      </div>

    </div>
  </div>
    
    <br>
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
      <!-- Latent space editing applications -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 has-text-centered">Related Works</h2>
          <div class="content has-text-justified">
            <p>
              <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. <a href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis with latent diffusion models.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.</li><br>
              <li>Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. <a href="https://arxiv.org/abs/2307.01952">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis.</a> In International Conference on Learning Representation (ICLR), 2024.</li><br>
              <li>Markus Kettunen, Erik Härkönen, and Jaakko Lehtinen. <a href="https://arxiv.org/abs/1906.03973">E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles.</a> In arXiv preprint arXiv:1906.03973, 2019.</li><br>
              <li>Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. <a href="https://arxiv.org/abs/2311.18828">One-step Diffusion with Distribution Matching Distillation.</a> In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</li><br>
              <li>Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. <a href="https://arxiv.org/abs/2311.17042">Adversarial Diffusion Distillation.</a> In arXiv preprint arXiv:2311.17042, 2023.</li><br>
              <li>Shanchuan Lin, Anran Wang, and Xiao Yang. <a href="https://arxiv.org/abs/2402.13929">SDXL-Lightning: Progressive Adversarial Diffusion Distillation.</a> 	In arXiv preprint arXiv:2402.13929, 2024.</li><br>  
            </p>
          </div>
        </div>
      </div> -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <br>
        <h2 class="title is-3 has-text-centered">Acknowledgements</h2>

        <div class="content has-text-justified">
          <p>
            This work is supported by NSFC (No.62206173), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), 
            MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (KLIP-HuMaCo).
            This work is also partially supported by HKU-SCF FinTech Academy,
            HKRGC Theme-based research scheme project T35-710/20-R, and SZ-HK-Macau Technology Research Programme #SGDX20210823103537030.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@inproceedings{yang2025unidemoire,
  author    = {Zemin Yang, Yujing Sun, Xidong Peng, Siu Ming Yiu, Yuexin Ma},
  title     = {UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis},
  booktitle = {Association for the Advancement of Artificial Intelligence (AAAI)},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="juxtapose/js/juxtapose.js"></script>

<script>
var slider;
let origImages = [
  {"src": "./static/images/1_output.jpeg", "label": "Moiré Image(Synthesized by our UniDemoiré)",},
  {"src": "./static/images/1_input.jpeg", "label": "Pure Moiré Pattern",},
  
];
let origOptions = {
    "makeResponsive": true,
    "showLabels": true,
    "mode": "horizontal",
    "showCredits": true,
    "animate": true,
    "startingPosition": "50"
};

const juxtaposeSelector = "#juxtapose-embed";
const transientSelector = "#juxtapose-hidden";


function tab_gallery_click(name) {
  // Get the expanded image
  let inputImage = {
    label: "Clean Image",
  };
  let outputImage = {
    label: "Pure Moiré Pattern (Synthesized by our UniDemoiré)",
  };

  inputImage.src = "./static/images/".concat(name, "_input.jpeg")
  outputImage.src = "./static/images/".concat(name, "_output.jpeg")

  // let images = [inputImage, outputImage];
  let images = [outputImage, inputImage];
  let options = slider.options;
  options.callback = function(obj) {
      var newNode = document.getElementById(obj.selector.substring(1));
      var oldNode = document.getElementById(juxtaposeSelector.substring(1));
      console.log(obj.selector.substring(1));
      console.log(newNode.children[0]);
      oldNode.replaceChild(newNode.children[0], oldNode.children[0]);
      //newNode.removeChild(newNode.children[0]);
      
  };
  
  slider = new juxtapose.JXSlider(transientSelector, images, options);
};



(function() {
    slider = new juxtapose.JXSlider(
        juxtaposeSelector, origImages, origOptions);
    //document.getElementById("left-button").onclick = replaceLeft;
    //document.getElementById("right-button").onclick = replaceRight;
})();
  // Get the image text
  var imgText = document.getElementById("imgtext");
  // Use the same src in the expanded image as the image being clicked on from the grid
  // expandImg.src = imgs.src;
  // Use the value of the alt attribute of the clickable image as text inside the expanded image
  imgText.innerHTML = name;
  // Show the container element (hidden with CSS)
  // expandImg.parentElement.style.display = "block";

$(".flip-card").click(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("out");
            $(div_front).removeClass("in");

            $(div_back).addClass("in");
            $(div_back).removeClass("out");

});

$(".flip-card").mouseleave(function() {
            console.log("fading in")
            div_back = $(this).children().children()[1]
            div_front = $(this).children().children()[0]
            // console.log($(this).children("div.flip-card-back"))
            console.log(div_back)
            $(div_front).addClass("in");
            $(div_front).removeClass("out");

            $(div_back).addClass("out");
            $(div_back).removeClass("in");

});

</script>
<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

</body>
</html>
